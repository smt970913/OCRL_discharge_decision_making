{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCRL Agent Set-up & Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import shap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()\n",
    "# torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.get_num_threads(): 128\n",
      "torch.get_num_interop_threads(): 128\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.get_num_threads():\", torch.get_num_threads())\n",
    "print(\"torch.get_num_interop_threads():\", torch.get_num_interop_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_id = 0 \n",
    "# props = torch.cuda.get_device_properties(device_id)\n",
    "# total_memory = props.total_memory  \n",
    "\n",
    "# print(\"Total GPU memory (bytes):\", total_memory)\n",
    "# print(\"Total GPU memory (GB):\", total_memory / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up OCRL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_path = str(Path().absolute())\n",
    "parent_path = str(Path().absolute().parent)\n",
    "sys.path.append(parent_path) # add current terminal path to sys.path\n",
    "curr_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # obtain current time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_fqe(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "\n",
    "        super(FCN_fqe, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_fqi(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "\n",
    "        super(FCN_fqi, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, obj_cost, con_cost, next_state, done):\n",
    "\n",
    "        if not isinstance(con_cost, list) and not isinstance(con_cost, tuple):\n",
    "            con_cost = [con_cost]\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "\n",
    "        self.buffer[self.position] = (state, action, obj_cost, con_cost, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, obj_cost, con_cost, next_state, done = zip(*batch)\n",
    "\n",
    "        con_cost = [list(costs) for costs in zip(*con_cost)]\n",
    "\n",
    "        return state, action, obj_cost, con_cost, next_state, done\n",
    "\n",
    "    def extract(self):\n",
    "        batch = self.buffer\n",
    "        state, action, obj_cost, con_cost, next_state, done = zip(*batch)\n",
    "\n",
    "        con_cost = [list(costs) for costs in zip(*con_cost)]\n",
    "\n",
    "        return state, action, obj_cost, con_cost, next_state, done\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FQE:\n",
    "    def __init__(self, cfg, state_dim, action_dim, id_stop, eval_agent, weight_decay, eval_target = 'obj'):\n",
    "\n",
    "        self.device = cfg.device\n",
    "\n",
    "        self.gamma = cfg.gamma\n",
    "\n",
    "        ### indicate optimal stopping structure or not\n",
    "        self.id_stop = id_stop\n",
    "\n",
    "        ### For constraint cost, specify which constraint to evaluate\n",
    "        if eval_target == 'obj':\n",
    "            self.lr_fqe = cfg.lr_fqe_obj\n",
    "        else:\n",
    "            self.lr_fqe = cfg.lr_fqe_con[eval_target] \n",
    "\n",
    "        # define policy Q-Estimator\n",
    "        self.policy_net = FCN_fqe(state_dim, action_dim).to(self.device)\n",
    "        # define target Q-Estimator\n",
    "        self.target_net = FCN_fqe(state_dim, action_dim).to(self.device)\n",
    "\n",
    "        # initialize target Q-Estimator with policy Q-Estimator\n",
    "        for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "        self.optimizer = optim.SGD(self.policy_net.parameters(), lr = self.lr_fqe)\n",
    "        # self.optimizer = optim.SGD([\n",
    "        #     {'params': self.policy_net.fc1.weight, 'weight_decay': self.weight_decay}, \n",
    "        #     {'params': self.policy_net.fc1.bias,   'weight_decay': 0.0}\n",
    "        #     ], lr = self.lr_fqe)\n",
    "        # self.optimizer = optim.Adam(self.policy_net.parameters(), lr = self.lr_fqe)\n",
    "        \n",
    "        # define loss function\n",
    "        self.loss = cfg.loss_fqe\n",
    "        \n",
    "        # input the evaluation agent\n",
    "        self.eval_agent = eval_agent\n",
    "\n",
    "    def update(self, state_batch, action_batch, cost_batch, next_state_batch, done_batch, disch_batch):\n",
    "\n",
    "        # We need to evaluate the parameterized policy\n",
    "        policy_action_batch = self.eval_agent.rl_policy(next_state_batch)\n",
    "\n",
    "        # predicted Q-value using policy Q-network\n",
    "        q_values = self.policy_net(state_batch).gather(dim = 1, index = action_batch)\n",
    "\n",
    "        # target Q-value calculated by target Q-network\n",
    "        next_q_values = self.target_net(next_state_batch).gather(dim = 1, index = policy_action_batch).squeeze(1).detach()\n",
    "        \n",
    "        if self.id_stop == 0:\n",
    "            expected_q_values = cost_batch + self.gamma * next_q_values * (1 - done_batch)\n",
    "        else:\n",
    "            expected_q_values = cost_batch + self.gamma * next_q_values * (1 - disch_batch)\n",
    "            \n",
    "        loss = self.loss(q_values, expected_q_values.unsqueeze(1))\n",
    "\n",
    "        # Update reward Q-network by minimizing the above loss function\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def avg_Q_value_est(self, state_batch):\n",
    "        policy_action_batch = self.eval_agent.rl_policy(state_batch)\n",
    "        q_values = self.policy_net(state_batch).gather(dim = 1, index = policy_action_batch).squeeze(1)\n",
    "\n",
    "        q_mean = q_values.mean()\n",
    "        q_std = q_values.std()\n",
    "        n = q_values.shape[0]\n",
    "    \n",
    "        if n <= 1 or q_std == 0:\n",
    "            return q_mean.item(), q_mean.item()\n",
    "    \n",
    "        z = 2.33  \n",
    "        q_upper_bound = q_mean + z * (q_std / math.sqrt(n))\n",
    "    \n",
    "        return q_mean.item(), q_upper_bound.item()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.policy_net.state_dict(), path + 'FQE_policy_network.pth')\n",
    "        torch.save(self.target_net.state_dict(), path + 'FQE_target_network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FQI:\n",
    "    def __init__(self, cfg, state_dim, action_dim):\n",
    "        \n",
    "        self.device = cfg.device\n",
    "        self.gamma = cfg.gamma\n",
    "        \n",
    "        self.lr = cfg.lr_fqi\n",
    "\n",
    "        self.policy_net = FCN_fqi(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = FCN_fqi(state_dim, action_dim).to(self.device)\n",
    "\n",
    "        for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        # self.optimizer = optim.SGD(self.policy_net.parameters(), lr = self.lr)\n",
    "        self.optimizer = optim.SGD([\n",
    "            {'params': self.policy_net.fc1.weight, 'weight_decay': 1e-2}, \n",
    "            {'params': self.policy_net.fc1.bias,   'weight_decay': 0.0}\n",
    "            ], lr = self.lr)\n",
    "        # self.optimizer = optim.Adam(self.policy_net.parameters(), lr = self.lr)\n",
    "\n",
    "        self.loss = cfg.loss_fqi\n",
    "\n",
    "    def update(self, lambda_t_list, state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch, disch_batch):\n",
    "\n",
    "        q_values = self.policy_net(state_batch).gather(dim = 1, index = action_batch)\n",
    "        policy_action_batch = self.policy_net(next_state_batch).min(1)[1].unsqueeze(1)\n",
    "        next_q_values = self.target_net(next_state_batch).gather(dim = 1, index = policy_action_batch).squeeze(1).detach()\n",
    "\n",
    "        sum_con_cost = 0\n",
    "        for i in range(len(lambda_t_list)):\n",
    "            lambda_t = lambda_t_list[i]\n",
    "            sum_con_cost += lambda_t * con_cost_batch[i]\n",
    "\n",
    "        expected_q_values = (obj_cost_batch + sum_con_cost) + self.gamma * next_q_values * (1 - done_batch)\n",
    "        # expected_q_values = (obj_cost_batch + sum_con_cost) + self.gamma * next_q_values * (1 - disch_batch)\n",
    "\n",
    "        loss = self.loss(q_values, expected_q_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def avg_Q_value_est(self, state_batch):\n",
    "\n",
    "        q_values = self.policy_net(state_batch)\n",
    "        avg_q_values = q_values.min(1)[0].unsqueeze(1).mean().item()\n",
    "\n",
    "        return avg_q_values\n",
    "\n",
    "    def rl_policy(self, state_batch):\n",
    "\n",
    "        q_values = self.policy_net(state_batch)\n",
    "        policy_action_batch = q_values.min(1)[1].unsqueeze(1)\n",
    "\n",
    "        return policy_action_batch\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.policy_net.state_dict(), path + 'Offline_FQI_policy_network.pth')\n",
    "        torch.save(self.target_net.state_dict(), path + 'Offline_FQI_target_network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLConfig:\n",
    "    def __init__(self, algo_name, train_eps, gamma, lr_fqi, lr_fqe_obj, constraint_num, lr_fqe_con_list, lr_lambda_list, threshold_list):\n",
    "        \n",
    "        self.algo = algo_name  # name of algorithm\n",
    "\n",
    "        self.train_eps = train_eps  #the number of trainng episodes\n",
    "\n",
    "        self.gamma = gamma # discount factor\n",
    "        \n",
    "        self.constraint_num = constraint_num\n",
    "\n",
    "        # learning rates\n",
    "        self.lr_fqi = lr_fqi\n",
    "        self.lr_fqe_obj = lr_fqe_obj\n",
    "        self.lr_fqe_con = [0 for i in range(constraint_num)]\n",
    "        self.lr_lam = [0 for i in range(constraint_num)]\n",
    "\n",
    "        # constraint threshold\n",
    "        self.constraint_limit = [0 for i in range(constraint_num)]\n",
    "        for i in range(constraint_num):\n",
    "            self.lr_fqe_con[i] = lr_fqe_con_list[i]\n",
    "            self.lr_lam[i] = lr_lambda_list[i]\n",
    "            self.constraint_limit[i] = threshold_list[i]\n",
    "\n",
    "        self.train_eps_steps = int(1e3)  # the number of steps in each training episode\n",
    "\n",
    "        self.batch_size = 256\n",
    "\n",
    "        self.loss_fqi = nn.MSELoss()\n",
    "        self.loss_fqe = nn.MSELoss()\n",
    "\n",
    "        self.memory_capacity = int(2e6)  # capacity of Replay Memory\n",
    "\n",
    "        self.target_update = 100 # update frequency of target net\n",
    "        self.tau = 0.01\n",
    "\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # check GPU\n",
    "        self.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, cfg, state_id_table, rl_cont_state_table, rl_cont_state_table_scaled, terminal_state):\n",
    "        self.cfg = cfg\n",
    "    \n",
    "        # Load datasets\n",
    "        self.state_df_id = state_id_table\n",
    "        self.rl_cont_state_table = rl_cont_state_table\n",
    "        self.rl_cont_state_table_scaled = rl_cont_state_table_scaled\n",
    "\n",
    "        self.terminal_state = terminal_state\n",
    "\n",
    "    def data_buffer_train(self, num_constraint = 2):\n",
    "        self.train_memory = ReplayBuffer(self.cfg.memory_capacity)\n",
    "\n",
    "        for i in range(len(self.state_df_id)):\n",
    "            state = self.rl_cont_state_table_scaled.values[i]\n",
    "            action = self.state_df_id['discharge_action'].values[i]\n",
    "            \n",
    "            if action == 1.0:\n",
    "                if self.state_df_id['death'].values[i] == 1.0:\n",
    "                    if self.state_df_id['discharge_fail'].values[i] == 1.0:\n",
    "                        done = 0.0\n",
    "                    else:\n",
    "                        done = 1.0\n",
    "                else:\n",
    "                    if self.state_df_id['discharge_fail'].values[i] == 0.0:\n",
    "                        done = 1.0\n",
    "                    else:\n",
    "                        done = 0.0\n",
    "            else:\n",
    "                done = 0.0\n",
    "            \n",
    "            obj_cost = self.state_df_id['mortality_costs_md'].values[i]\n",
    "            con_cost = []\n",
    "            \n",
    "            for j in range(num_constraint):\n",
    "                cost_col = f'con_cost_{j}'  \n",
    "                if cost_col in self.state_df_id.columns:\n",
    "                    con_cost.append(self.state_df_id[cost_col].values[i])\n",
    "                else:\n",
    "                    con_cost.append(0.0) \n",
    "\n",
    "            if done == 0.0:\n",
    "                idx = self.state_df_id.index[i]\n",
    "                next_state = self.rl_cont_state_table_scaled.loc[idx + 1].values\n",
    "            else:\n",
    "                next_state = self.terminal_state\n",
    "\n",
    "            self.train_memory.push(state, action, obj_cost, con_cost, next_state, done)\n",
    "\n",
    "    def data_torch_loader_train(self):\n",
    "        state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch = self.train_memory.sample(self.cfg.batch_size)\n",
    "\n",
    "        state_batch = torch.tensor(np.array(state_batch), device = self.cfg.device, dtype = torch.float)\n",
    "\n",
    "        disch_batch = list(action_batch)\n",
    "        action_batch = torch.tensor(np.array(action_batch), device = self.cfg.device, dtype = torch.long).unsqueeze(1)\n",
    "        \n",
    "        obj_cost_batch = torch.tensor(np.array(obj_cost_batch), device = self.cfg.device, dtype = torch.float)\n",
    "        con_cost_batch = [torch.tensor(np.array(cost), device = self.cfg.device, dtype=torch.float) for cost in con_cost_batch]\n",
    "        next_state_batch = torch.tensor(np.array(next_state_batch), device = self.cfg.device, dtype = torch.float)\n",
    "        \n",
    "        done_batch = torch.tensor(np.array(done_batch), device = self.cfg.device, dtype = torch.float)\n",
    "        disch_batch = torch.tensor(np.array(disch_batch), device = self.cfg.device, dtype = torch.float)\n",
    "\n",
    "        return state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch, disch_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataLoader:\n",
    "    def __init__(self, cfg, state_id_table, rl_cont_state_table, rl_cont_state_table_scaled, state_id_table_1, rl_cont_state_table_scaled_1, terminal_state):\n",
    "        self.cfg = cfg\n",
    "    \n",
    "        # Load datasets\n",
    "        self.state_df_id = state_id_table\n",
    "        self.rl_cont_state_table = rl_cont_state_table\n",
    "        self.rl_cont_state_table_scaled = rl_cont_state_table_scaled\n",
    "\n",
    "        self.state_df_id_1 = state_id_table_1\n",
    "        self.rl_cont_state_table_scaled_1 = rl_cont_state_table_scaled_1\n",
    "\n",
    "        self.terminal_state = terminal_state\n",
    "\n",
    "    def data_buffer_val(self, num_constraint = 2):\n",
    "        self.val_memory = ReplayBuffer(self.cfg.memory_capacity)\n",
    "\n",
    "        for i in range(len(self.state_df_id)):\n",
    "            state = self.rl_cont_state_table_scaled.values[i]\n",
    "            action = self.state_df_id['discharge_action'].values[i]\n",
    "            \n",
    "            if action == 1.0:\n",
    "                if self.state_df_id['death'].values[i] == 1.0:\n",
    "                    if self.state_df_id['discharge_fail'].values[i] == 1.0:\n",
    "                        done = 0.0\n",
    "                    else:\n",
    "                        done = 1.0\n",
    "                else:\n",
    "                    if self.state_df_id['discharge_fail'].values[i] == 0.0:\n",
    "                        done = 1.0\n",
    "                    else:\n",
    "                        done = 0.0\n",
    "            else:\n",
    "                done = 0.0\n",
    "            \n",
    "            obj_cost = self.state_df_id['mortality_costs_md'].values[i]\n",
    "            con_cost = []\n",
    "            \n",
    "            for j in range(num_constraint):\n",
    "                cost_col = f'con_cost_{j}'  \n",
    "                if cost_col in self.state_df_id.columns:\n",
    "                    con_cost.append(self.state_df_id[cost_col].values[i])\n",
    "                else:\n",
    "                    con_cost.append(0.0) \n",
    "\n",
    "            if done == 0.0:\n",
    "                idx = self.state_df_id.index[i]\n",
    "                next_state = self.rl_cont_state_table_scaled_1.loc[idx + 1].values\n",
    "            else:\n",
    "                next_state = self.terminal_state\n",
    "\n",
    "            self.val_memory.push(state, action, obj_cost, con_cost, next_state, done)\n",
    "\n",
    "    def data_torch_loader_val(self):\n",
    "        state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch = self.val_memory.extract()\n",
    "\n",
    "        state_batch = torch.tensor(np.array(state_batch), device = self.cfg.device, dtype = torch.float)\n",
    "\n",
    "        return state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLTraining:\n",
    "    def __init__(self, cfg, state_dim, action_dim, val_data_loader, data_loader):\n",
    "        self.cfg = cfg\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.val_data_loader = val_data_loader\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        # Store for FQI models history\n",
    "        self.fqi_models_history = []\n",
    "        \n",
    "        # Store for FQE models history\n",
    "        self.fqe_obj_models_history = []\n",
    "        self.fqe_con_models_history = {}\n",
    "\n",
    "    def fqi_agent_config(self, seed = 1):\n",
    "        agent_fqi = FQI(self.cfg, self.state_dim, self.action_dim)\n",
    "        torch.manual_seed(seed)\n",
    "        return agent_fqi\n",
    "\n",
    "    def fqe_agent_config(self, id_stop, eval_agent, weight_decay, eval_target, seed = 2):\n",
    "        agent_fqe = FQE(self.cfg, self.state_dim, self.action_dim, id_stop, eval_agent, weight_decay, eval_target)\n",
    "        torch.manual_seed(seed)\n",
    "        return agent_fqe\n",
    "\n",
    "    def train(self, agent_fqi, agent_fqe_obj, agent_fqe_con_list, constraint = None):\n",
    "        print('Start to train!')\n",
    "        print(f'Algorithm:{self.cfg.algo}, Device:{self.cfg.device}')\n",
    "\n",
    "        self.FQI_loss = []\n",
    "        self.FQI_est_values = []\n",
    "\n",
    "        self.FQE_loss_obj = []\n",
    "        self.FQE_loss_con = {i: [] for i in range(len(agent_fqe_con_list))}\n",
    "\n",
    "        self.FQE_est_obj_costs = []\n",
    "        self.FQE_est_con_costs = {i: [] for i in range(len(agent_fqe_con_list))}\n",
    "\n",
    "        self.lambda_dict = {i: [] for i in range(len(agent_fqe_con_list))}\n",
    "        \n",
    "        # Initialize the model history dictionaries for constraint agents\n",
    "        for i in range(len(agent_fqe_con_list)):\n",
    "            self.fqe_con_models_history[i] = []\n",
    "\n",
    "        lambda_t_list = [0 for i in range(len(agent_fqe_con_list))]\n",
    "        lambda_update_list = [0 for i in range(len(agent_fqe_con_list))]\n",
    "\n",
    "        state_batch_val = self.val_data_loader()\n",
    "        \n",
    "        model_update_counter = 0  # Counter to track model updates\n",
    "\n",
    "        for k in range(self.cfg.train_eps):\n",
    "            loss_list_fqi = []\n",
    "            loss_list_fqe_obj = []\n",
    "            loss_list_fqe_con = {i: [] for i in range(len(agent_fqe_con_list))}\n",
    "\n",
    "            fqi_est_list = []\n",
    "            fqe_est_obj = []\n",
    "            fqe_est_con = {i: [] for i in range(len(agent_fqe_con_list))}\n",
    "\n",
    "            for j in tqdm(range(self.cfg.train_eps_steps)):\n",
    "\n",
    "                state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch, disch_batch = self.data_loader()\n",
    "                \n",
    "                # update the policy agent for learning agent (FQI) and evaluation agent (FQE)\n",
    "                loss_rl = agent_fqi.update(lambda_t_list, state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch, disch_batch)\n",
    "                loss_ev_obj = agent_fqe_obj.update(state_batch, action_batch, obj_cost_batch, next_state_batch, done_batch, disch_batch)\n",
    "                \n",
    "                # Save FQI model state\n",
    "                model_update_counter += 1\n",
    "                if len(self.fqi_models_history) >= 2000:\n",
    "                    self.fqi_models_history.pop(0)  \n",
    "                self.fqi_models_history.append({\n",
    "                    'update_num': model_update_counter,\n",
    "                    'epoch': k,\n",
    "                    'step': j,\n",
    "                    'model_state': self._get_model_state(agent_fqi)\n",
    "                })\n",
    "\n",
    "                # Save FQE objective model state\n",
    "                if len(self.fqe_obj_models_history) >= 2000:\n",
    "                    self.fqe_obj_models_history.pop(0) \n",
    "                self.fqe_obj_models_history.append({\n",
    "                    'update_num': model_update_counter,\n",
    "                    'epoch': k,\n",
    "                    'step': j,\n",
    "                    'model_state': self._get_model_state(agent_fqe_obj)\n",
    "                })\n",
    "                \n",
    "                ##############################################################################################################\n",
    "                loss_list_fqi.append(loss_rl)\n",
    "                loss_list_fqe_obj.append(loss_ev_obj)\n",
    "\n",
    "                if constraint == None:\n",
    "                    for m in range(len(agent_fqe_con_list)):\n",
    "                        loss_con = agent_fqe_con_list[m].update(state_batch, action_batch, con_cost_batch[m], next_state_batch, done_batch, disch_batch)\n",
    "                        loss_list_fqe_con[m].append(loss_con)\n",
    "                        \n",
    "                        # Save FQE constraint model state\n",
    "                        if len(self.fqe_con_models_history[m]) >= 2000:\n",
    "                            self.fqe_con_models_history[m].pop(0)  \n",
    "                        self.fqe_con_models_history[m].append({\n",
    "                            'update_num': model_update_counter,\n",
    "                            'epoch': k,\n",
    "                            'step': j,\n",
    "                            'model_state': self._get_model_state(agent_fqe_con_list[m])\n",
    "                        })\n",
    "                        \n",
    "                        con_est_value, con_est_value_up = agent_fqe_con_list[m].avg_Q_value_est(state_batch_val)\n",
    "                        fqe_est_con[m].append(con_est_value)\n",
    "                    \n",
    "                    fqi_est_value = agent_fqi.avg_Q_value_est(state_batch_val)\n",
    "                    avg_q_value_obj, avg_q_value_obj_up = agent_fqe_obj.avg_Q_value_est(state_batch_val)\n",
    "\n",
    "                    fqi_est_list.append(fqi_est_value)\n",
    "                    fqe_est_obj.append(avg_q_value_obj)\n",
    "\n",
    "                    lambda_update_list = [0 for i in range(len(agent_fqe_con_list))]\n",
    "                    lambda_t_list = [0 for i in range(len(agent_fqe_con_list))]\n",
    "                \n",
    "                else:\n",
    "                    for m in range(len(agent_fqe_con_list)):\n",
    "                        loss_con = agent_fqe_con_list[m].update(state_batch, action_batch, con_cost_batch[m], next_state_batch, done_batch, disch_batch)\n",
    "                        loss_list_fqe_con[m].append(loss_con)\n",
    "                        \n",
    "                        # Save FQE constraint model state\n",
    "                        if len(self.fqe_con_models_history[m]) >= 2000:\n",
    "                            self.fqe_con_models_history[m].pop(0)\n",
    "                            \n",
    "                        self.fqe_con_models_history[m].append({\n",
    "                            'update_num': model_update_counter,\n",
    "                            'epoch': k,\n",
    "                            'step': j,\n",
    "                            'model_state': self._get_model_state(agent_fqe_con_list[m])\n",
    "                        })\n",
    "                        \n",
    "                        con_est_value, con_est_value_up = agent_fqe_con_list[m].avg_Q_value_est(state_batch_val)\n",
    "                        fqe_est_con[m].append(con_est_value)\n",
    "                        \n",
    "                        lambda_update_list[m] = con_est_value_up - self.cfg.constraint_limit[m]\n",
    "                        lambda_t_list[m] = lambda_t_list[m] + (self.cfg.lr_lam[m] * lambda_update_list[m])\n",
    "                        lambda_t_list[m] = max(0, lambda_t_list[m])\n",
    "                    \n",
    "                    fqi_est_value = agent_fqi.avg_Q_value_est(state_batch_val)\n",
    "                    avg_q_value_obj, avg_q_value_obj_up = agent_fqe_obj.avg_Q_value_est(state_batch_val)\n",
    "\n",
    "                    fqi_est_list.append(fqi_est_value)\n",
    "                    fqe_est_obj.append(avg_q_value_obj)\n",
    "                ######################################################################################\n",
    "                if j % self.cfg.target_update == 0:\n",
    "\n",
    "                    ### update the target agent for learning agent (FQI)\n",
    "                    for target_param, policy_param in zip(agent_fqi.target_net.parameters(), agent_fqi.policy_net.parameters()):\n",
    "                        target_param.data.copy_(self.cfg.tau * policy_param.data + (1 - self.cfg.tau) * target_param.data)\n",
    "\n",
    "                    ### update the target agent for evaluation agent (FQE objective cost)\n",
    "                    for target_param, policy_param in zip(agent_fqe_obj.target_net.parameters(), agent_fqe_obj.policy_net.parameters()):\n",
    "                        target_param.data.copy_(self.cfg.tau * policy_param.data + (1 - self.cfg.tau) * target_param.data)\n",
    "\n",
    "                    ### update the target agent for evaluation agent (FQE constraint cost)\n",
    "                    for agent_fqe_con in agent_fqe_con_list:\n",
    "                        for target_param, policy_param in zip(agent_fqe_con.target_net.parameters(), agent_fqe_con.policy_net.parameters()):\n",
    "                            target_param.data.copy_(self.cfg.tau * policy_param.data + (1 - self.cfg.tau) * target_param.data)\n",
    "                #########################################################################################\n",
    "            print(f\"Epoch {k + 1}/{self.cfg.train_eps}\")\n",
    "            print(f\"Average FQE estimated objective cost after epoch {k + 1}: {np.mean(fqe_est_obj)}\")        \n",
    "\n",
    "            for m in range(len(agent_fqe_con_list)):\n",
    "                print(f\"Average FQE estimated constraint cost of constraint {m} after epoch {k + 1}: {np.mean(fqe_est_con[m])}\")\n",
    "                print(f\"Dual variable of constraint {m} after epoch {k + 1}: {lambda_t_list[m]}\")\n",
    "                print(f\"Dual variable update of constraint {m} after epoch {k + 1}: {lambda_update_list[m]}\")\n",
    "\n",
    "                self.lambda_dict[m].append(lambda_t_list[m])\n",
    "                self.FQE_loss_con[m].append(np.mean(loss_list_fqe_con[m]))\n",
    "                self.FQE_est_con_costs[m].append(np.mean(fqe_est_con[m]))\n",
    "\n",
    "            self.FQI_loss.append(np.mean(loss_list_fqi))\n",
    "            self.FQE_loss_obj.append(np.mean(loss_list_fqe_obj))\n",
    "\n",
    "            self.FQI_est_values.append(np.mean(fqi_est_list))\n",
    "            self.FQE_est_obj_costs.append(np.mean(fqe_est_obj))\n",
    "\n",
    "        print(\"Complete Training!\")\n",
    "\n",
    "        return self.FQI_loss, self.FQE_loss_obj, self.FQE_loss_con, self.FQI_est_values, self.FQE_est_obj_costs, self.FQE_est_con_costs, self.lambda_dict\n",
    "    \n",
    "    def _get_model_state(self, agent):\n",
    "        \"\"\"\n",
    "        Extract the model state from an agent.\n",
    "        Returns a deep copy of both policy_net and target_net states.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'policy_net': copy.deepcopy(agent.policy_net.state_dict()),\n",
    "            'target_net': copy.deepcopy(agent.target_net.state_dict())\n",
    "        }\n",
    "    \n",
    "    def _save_models_to_disk(self):\n",
    "        \"\"\"\n",
    "        Save the models to disk.\n",
    "        This method can be customized based on your specific requirements.\n",
    "        \"\"\"       \n",
    "        # Create directory for saved models if it doesn't exist\n",
    "        os.makedirs('saved_models/fqe_obj', exist_ok = True)\n",
    "        \n",
    "        # Save objective FQE models\n",
    "        for idx, model_data in enumerate(self.fqe_obj_models_history[-2000:]):\n",
    "            torch.save(\n",
    "                model_data['model_state'], \n",
    "                f'saved_models/fqe_obj/model_{model_data[\"update_num\"]}.pt'\n",
    "            )\n",
    "        \n",
    "        # Save constraint FQE models\n",
    "        for con_idx in self.fqe_con_models_history.keys():\n",
    "            os.makedirs(f'saved_models/fqe_con_{con_idx}', exist_ok = True)\n",
    "            \n",
    "            for idx, model_data in enumerate(self.fqe_con_models_history[con_idx][-2000:]):\n",
    "                torch.save(\n",
    "                    model_data['model_state'], \n",
    "                    f'saved_models/fqe_con_{con_idx}/model_{model_data[\"update_num\"]}.pt'\n",
    "                )\n",
    "        \n",
    "        print(\"Models saved to disk in 'saved_models/' directory\")\n",
    "    \n",
    "    def load_fqe_model(self, agent, model_path):\n",
    "        \"\"\"\n",
    "        Load a saved FQE model into an agent.\n",
    "        \n",
    "        Args:\n",
    "            agent: The FQE agent to load the model into\n",
    "            model_path: Path to the saved model state\n",
    "        \n",
    "        Returns:\n",
    "            The agent with loaded model\n",
    "        \"\"\"\n",
    "        model_state = torch.load(model_path)\n",
    "        agent.policy_net.load_state_dict(model_state['policy_net'])\n",
    "        agent.target_net.load_state_dict(model_state['target_net'])\n",
    "        return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_table_train = pd.read_csv('../data_output/id_table_train_v13.csv')\n",
    "rl_table_train = pd.read_csv('../data_output/rl_table_train_v13.csv')\n",
    "rl_table_train_scaled = pd.read_csv('../data_output/rl_table_train_scaled_v13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_table_val = pd.read_csv('../data_output/id_table_val_v13.csv')\n",
    "rl_table_val = pd.read_csv('../data_output/rl_table_val_v13.csv')\n",
    "rl_table_val_scaled = pd.read_csv('../data_output/rl_table_val_scaled_v13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_table_train_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_table_train.drop(columns = ['readmission_count'], inplace = True)\n",
    "# rl_table_train_scaled.drop(columns = ['readmission_count'], inplace = True)\n",
    "# rl_table_val.drop(columns = ['readmission_count'], inplace = True)\n",
    "# rl_table_val_scaled.drop(columns = ['readmission_count'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_table_train_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the 'mortality_costs' column\n",
    "id_table_train['mortality_costs_md'] = id_table_train['mortality_costs'].copy()\n",
    "\n",
    "# Apply the conditions using vectorized operations\n",
    "discharge_action_mask = id_table_train['discharge_action'] == 1\n",
    "death_mask = id_table_train['death'] == 0\n",
    "discharge_fail_mask = id_table_train['discharge_fail'] == 1\n",
    "\n",
    "# Update 'mortality_costs_md' based on the conditions\n",
    "id_table_train.loc[discharge_action_mask & death_mask, 'mortality_costs_md'] = 0\n",
    "id_table_train.loc[discharge_action_mask & ~death_mask & discharge_fail_mask, 'mortality_costs_md'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the 'mortality_costs' column\n",
    "id_table_val['mortality_costs_md'] = id_table_val['mortality_costs'].copy()\n",
    "\n",
    "# Apply the conditions using vectorized operations\n",
    "discharge_action_mask = id_table_val['discharge_action'] == 1\n",
    "death_mask = id_table_val['death'] == 0\n",
    "discharge_fail_mask = id_table_val['discharge_fail'] == 1\n",
    "\n",
    "# Update 'mortality_costs_md' based on the conditions\n",
    "id_table_val.loc[discharge_action_mask & death_mask, 'mortality_costs_md'] = 0\n",
    "id_table_val.loc[discharge_action_mask & ~death_mask & discharge_fail_mask, 'mortality_costs_md'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_table_train['con_cost_0'] = id_table_train['discharge_fail_costs'].copy()\n",
    "id_table_train['con_cost_1'] = id_table_train['los_costs_scaled'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_table_val['con_cost_0'] = id_table_val['discharge_fail_costs'].copy()\n",
    "id_table_val['con_cost_1'] = id_table_val['los_costs_scaled'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = RLConfig(algo_name = 'OCRL_v2_trainset_104', train_eps = int(9e3), gamma = 1.0, lr_fqi = 2e-3, lr_fqe_obj = 5e-4, \n",
    "               constraint_num = 2, lr_fqe_con_list = [5e-4, 5e-4], lr_lambda_list = [3e-4, 1e-6], threshold_list = [0.14, 4.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_state = np.zeros(rl_table_train_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(cfg, id_table_train, rl_table_train, rl_table_train_scaled, terminal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader.data_buffer_train(num_constraint = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_table_val_initial = id_table_val[id_table_val['readmission_count'] == 0].copy()\n",
    "id_table_val_initial = id_table_val_initial[id_table_val_initial['epoch'] == 1].copy()\n",
    "\n",
    "mv_val_initial_index = id_table_val_initial.index\n",
    "id_index_list_initial_val = mv_val_initial_index.tolist()\n",
    "\n",
    "rl_table_val_initial = rl_table_val.loc[id_index_list_initial_val].copy()\n",
    "\n",
    "rl_table_val_scaled_initial = rl_table_val_scaled.loc[id_index_list_initial_val].copy()\n",
    "\n",
    "id_table_val_initial['con_cost_0'] = id_table_val_initial['discharge_fail_costs'].copy()\n",
    "id_table_val_initial['con_cost_1'] = id_table_val_initial['los_costs_scaled'].copy()\n",
    "\n",
    "val_data_loader = ValDataLoader(cfg, \n",
    "                                id_table_val_initial, \n",
    "                                rl_table_val_initial, \n",
    "                                rl_table_val_scaled_initial, \n",
    "                                id_table_val, \n",
    "                                rl_table_val_scaled, \n",
    "                                terminal_state)\n",
    "\n",
    "val_data_loader.data_buffer_val(num_constraint = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training OCRL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(16)\n",
    "torch.set_num_interop_threads(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrl_training_s1 = RLTraining(cfg, state_dim = rl_table_train_scaled.shape[1], \n",
    "                              action_dim = 2, \n",
    "                              val_data_loader = val_data_loader.data_torch_loader_val, \n",
    "                              data_loader = train_data_loader.data_torch_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_fqi_s1 = ocrl_training_s1.fqi_agent_config(seed = 1000)\n",
    "\n",
    "### objective cost: mortality risk\n",
    "agent_fqe_obj_s1 = ocrl_training_s1.fqe_agent_config(id_stop = 0, \n",
    "                                                     eval_agent = agent_fqi_s1, \n",
    "                                                     weight_decay = 0.0,\n",
    "                                                     eval_target = 'obj', \n",
    "                                                     seed = 2000) \n",
    "\n",
    "### constraint cost 1: readmission risk\n",
    "agent_fqe_con_rr_s1 = ocrl_training_s1.fqe_agent_config(id_stop = 0, \n",
    "                                                        eval_agent = agent_fqi_s1, \n",
    "                                                        weight_decay = 0.0,\n",
    "                                                        eval_target = 0, \n",
    "                                                        seed = 3000) \n",
    "\n",
    "### constraint cost 2: length-of-stay\n",
    "agent_fqe_con_los_s1 = ocrl_training_s1.fqe_agent_config(id_stop = 0, \n",
    "                                                         eval_agent = agent_fqi_s1, \n",
    "                                                         weight_decay = 0.0,\n",
    "                                                         eval_target = 1, \n",
    "                                                         seed = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ocrl_training_s1.train(agent_fqi_s1, \n",
    "                       agent_fqe_obj_s1, [agent_fqe_con_rr_s1, agent_fqe_con_los_s1], \n",
    "                       constraint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ocrl_models_and_data(agent_fqi_s1,\n",
    "                              agent_fqe_obj_s1,\n",
    "                              agent_fqe_con_rr_s1,\n",
    "                              agent_fqe_con_los_s1,\n",
    "                              ocrl_training_s1,\n",
    "                              approx_model = \"linear\",\n",
    "                              version = \"v1\"):\n",
    "    \"\"\"\n",
    "    Automatically generate the current date and version number, and save the model and the corresponding training data.\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    agent_fqi_s1 : trained FQI model object\n",
    "    agent_fqe_obj_s1 : trained FQE model object (objective function)\n",
    "    agent_fqe_con_rr_s1 : trained FQE model object (a constraint, e.g., readmission risk)\n",
    "    agent_fqe_con_los_s1 : trained FQE model object (another constraint, e.g., length-of-stay)\n",
    "    ocrl_training_s1 : training process object, including FQI_loss, FQI_est_values, etc.\n",
    "    version : model version number string (can be customized, default is \"v1\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Automatically generate the current date\n",
    "    date_str = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # Folder prefix (can be modified as needed)\n",
    "    model_prefix = \"../model_output\"\n",
    "    data_prefix = \"../model_output/data_output\"\n",
    "    \n",
    "    # ========= Save models =========\n",
    "    torch.save(agent_fqi_s1, f\"{model_prefix}/ocrl_agent_s1_fqi_{date_str}_{version}.pth\")\n",
    "    torch.save(agent_fqe_obj_s1, f\"{model_prefix}/ocrl_agent_s1_fqe_obj_{date_str}_{version}.pth\")\n",
    "    torch.save(agent_fqe_con_rr_s1, f\"{model_prefix}/ocrl_agent_s1_fqe_con_rr_{date_str}_{version}.pth\")\n",
    "    torch.save(agent_fqe_con_los_s1, f\"{model_prefix}/ocrl_agent_s1_fqe_con_los_{date_str}_{version}.pth\")\n",
    "\n",
    "    # ========= Save the results of the training process =========\n",
    "    np.save(f\"{data_prefix}/{approx_model}_fqi_loss_{date_str}_{version}.npy\", np.array(ocrl_training_s1.FQI_loss))\n",
    "    np.save(f\"{data_prefix}/{approx_model}_fqi_est_value_{date_str}_{version}.npy\", np.array(ocrl_training_s1.FQI_est_values))\n",
    "\n",
    "    np.save(f\"{data_prefix}/{approx_model}_fqe_obj_loss_{date_str}_{version}.npy\", np.array(ocrl_training_s1.FQE_loss_obj))\n",
    "    np.save(f\"{data_prefix}/{approx_model}_fqe_con_rr_loss_{date_str}_{version}.npy\", np.array(ocrl_training_s1.FQE_loss_con[0]))\n",
    "    np.save(f\"{data_prefix}/{approx_model}_fqe_con_los_loss_{date_str}_{version}.npy\", np.array(ocrl_training_s1.FQE_loss_con[1]))\n",
    "\n",
    "    np.save(f\"{data_prefix}/{approx_model}_fqe_est_obj_{date_str}_{version}.npy\", np.array(ocrl_training_s1.FQE_est_obj_costs))\n",
    "    np.save(f\"{data_prefix}/{approx_model}_fqe_est_con_rr_{date_str}_{version}.npy\", np.array(ocrl_training_s1.FQE_est_con_costs[0]))\n",
    "    np.save(f\"{data_prefix}/{approx_model}_fqe_est_con_los_{date_str}_{version}.npy\", np.array(ocrl_training_s1.FQE_est_con_costs[1]))\n",
    "\n",
    "    np.save(f\"{data_prefix}/{approx_model}_lambda_rr_{date_str}_{version}.npy\", np.array(ocrl_training_s1.lambda_dict[0]))\n",
    "    np.save(f\"{data_prefix}/{approx_model}_lambda_los_{date_str}_{version}.npy\", np.array(ocrl_training_s1.lambda_dict[1]))\n",
    "\n",
    "    print(f\"model and data saved, date: {date_str}, approximation model: {approx_model}, version: {version}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ocrl_models_and_data(\n",
    "    agent_fqi_s1 = agent_fqi_s1,\n",
    "    agent_fqe_obj_s1 = agent_fqe_obj_s1,\n",
    "    agent_fqe_con_rr_s1 = agent_fqe_con_rr_s1,\n",
    "    agent_fqe_con_los_s1 = agent_fqe_con_los_s1,\n",
    "    ocrl_training_s1 = ocrl_training_s1,\n",
    "    approx_model = \"linear\",\n",
    "    version = \"v0\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ocrl_training_s1.FQI_loss)\n",
    "\n",
    "plt.title(\"The Loss of Reinforcement Learning Agent\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_fqi_loss_20250331.png\", \n",
    "#             dpi = 300, \n",
    "#             bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ocrl_training_s1.FQI_est_values)\n",
    "\n",
    "plt.title(\"The estimated Q value of FQI agent\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel(\"Average estimated Q value\")\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_fqi_est_Q_20250331.png\", \n",
    "#             dpi = 300,\n",
    "#             bbox_inches = 'tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ocrl_training_s1.FQE_loss_obj)\n",
    "\n",
    "plt.title(\"The Loss of FQE (Objective Cost) Agent\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_fqe_obj_loss_20250331.png\", \n",
    "#             dpi = 300, \n",
    "#             bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ocrl_training_s1.FQE_loss_con[0])\n",
    "\n",
    "plt.title(\"The Loss of FQE (Constrained Costs) Agent\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_fqe_con_rdm_loss_20250331.png\", \n",
    "#             dpi = 300, \n",
    "#             bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ocrl_training_s1.FQE_loss_con[1])\n",
    "\n",
    "plt.title(\"The Loss of FQE (Constrained Costs) Agent\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_fqe_con_los_loss_20250331.png\", \n",
    "#             dpi = 300, \n",
    "#             bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ocrl_training_s1.FQE_est_obj_costs, label = r'OCRL, $l = (0.14, 60)$')\n",
    "\n",
    "last_epoch = len(ocrl_training_s1.FQE_est_obj_costs) - 1\n",
    "last_value = ocrl_training_s1.FQE_est_obj_costs[-1]\n",
    "\n",
    "plt.text(last_epoch + 1600, last_value, f'{last_value:.4f}', ha = 'right', va = 'center', fontsize = 12, color = 'red')\n",
    "\n",
    "plt.axhline(y = last_value, color = 'r', linestyle = '--')\n",
    "\n",
    "# plt.title(\"The Average Estimated Value of Objective Costs by FQE\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "# plt.ylabel(r\"Mortality Risk ($\\%$)\")\n",
    "plt.ylabel(r\"Mortality Risk\")\n",
    "plt.legend(prop = {'size': 8})\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_fqe_obj_est_value_20250331.png\", \n",
    "#             dpi = 300, \n",
    "#             bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ocrl_training_s1.FQE_est_con_costs[0], label = r'OCRL, $l = (0.14, 60)$')\n",
    "\n",
    "last_epoch = len(ocrl_training_s1.FQE_est_con_costs[0]) - 1\n",
    "last_value = ocrl_training_s1.FQE_est_con_costs[0][-1]\n",
    "\n",
    "# plt.text(last_epoch + 1800, last_value, f'{last_value:.2f}%', ha = 'right', va = 'center', fontsize = 12, color = 'red')\n",
    "plt.text(last_epoch + 1600, last_value, f'{last_value:.4f}', ha = 'right', va = 'center', fontsize = 12, color = 'red')\n",
    "\n",
    "plt.axhline(y = last_value, color = 'r', linestyle = '--')\n",
    "\n",
    "# plt.title(\"The Average Estimated Value of Constrained Costs by FQE\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "# plt.ylabel(r\"Readmission Risk ($\\%$)\")\n",
    "plt.ylabel(r\"Readmission Risk\")\n",
    "plt.legend(prop = {'size': 8})\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_fqe_con_disch_fail_est_value_20250331.png\", \n",
    "#             dpi = 300, \n",
    "#             bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(ocrl_training_s1.FQE_est_con_costs[1])*12, label = r'OCRL, $l = (0.14, 60)$')\n",
    "\n",
    "last_epoch = len(ocrl_training_s1.FQE_est_con_costs[1]) - 1\n",
    "last_value = ocrl_training_s1.FQE_est_con_costs[1][-1] * 12\n",
    "\n",
    "# plt.text(last_epoch + 1600, last_value, f'{last_value:.2f}', ha = 'right', va = 'center', fontsize = 12, color = 'red')\n",
    "\n",
    "# plt.axhline(y = last_value, color = 'r', linestyle = '--')\n",
    "plt.axhline(y = 60, color = 'g', linestyle = '-')\n",
    "\n",
    "\n",
    "# plt.title(\"The Average Estimated Value of Constrained Costs by FQE\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel(r\"Length-of-Stay (hrs)\")\n",
    "plt.legend(prop = {'size': 8})\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_fqe_con_los_value_20250331.png\", \n",
    "#             dpi = 300, \n",
    "#             bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ocrl_training_s1.lambda_dict[0], label = r'OCRL, $l = (0.14, 60)$')\n",
    "\n",
    "plt.title(r\"Training Process of Lagrangian Multiplier $\\lambda$\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "ylabel = plt.ylabel(r\"$\\lambda$\")\n",
    "ylabel.set_rotation(0)\n",
    "\n",
    "plt.legend(prop={'size': 8})\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_lambda_disch_fail_20250331.png\", \n",
    "#             dpi = 300, \n",
    "#             bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ocrl_training_s1.lambda_dict[1], label = r'OCRL, $l = (0.14, 60)$')\n",
    "\n",
    "plt.title(r\"Training Process of Lagrangian Multiplier $\\lambda$\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "ylabel = plt.ylabel(r\"$\\lambda$\")\n",
    "ylabel.set_rotation(0)\n",
    "\n",
    "plt.legend(prop={'size': 8})\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/ocrl_lambda_los_20250331.png\", \n",
    "#             dpi = 300, \n",
    "#             bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataLoader:\n",
    "    def __init__(self, cfg, state_id_table, rl_cont_state_table, rl_cont_state_table_scaled, state_id_table_1, rl_cont_state_table_scaled_1, terminal_state):\n",
    "        self.cfg = cfg\n",
    "    \n",
    "        # Load datasets\n",
    "        self.state_df_id = state_id_table\n",
    "        self.rl_cont_state_table = rl_cont_state_table\n",
    "        self.rl_cont_state_table_scaled = rl_cont_state_table_scaled\n",
    "\n",
    "        self.state_df_id_1 = state_id_table_1\n",
    "        self.rl_cont_state_table_scaled_1 = rl_cont_state_table_scaled_1\n",
    "\n",
    "        self.terminal_state = terminal_state\n",
    "\n",
    "    def data_buffer_test(self, num_constraint = 2):\n",
    "        self.test_memory = ReplayBuffer(self.cfg.memory_capacity)\n",
    "\n",
    "        for i in range(len(self.state_df_id)):\n",
    "            state = self.rl_cont_state_table_scaled.values[i]\n",
    "            action = self.state_df_id['discharge_action'].values[i]\n",
    "            \n",
    "            if action == 1.0:\n",
    "                if self.state_df_id['death'].values[i] == 1.0:\n",
    "                    if self.state_df_id['discharge_fail'].values[i] == 1.0:\n",
    "                        done = 0.0\n",
    "                    else:\n",
    "                        done = 1.0\n",
    "                else:\n",
    "                    if self.state_df_id['discharge_fail'].values[i] == 0.0:\n",
    "                        done = 1.0\n",
    "                    else:\n",
    "                        done = 0.0\n",
    "            else:\n",
    "                done = 0.0\n",
    "            \n",
    "            obj_cost = self.state_df_id['mortality_costs_md'].values[i]\n",
    "            con_cost = []\n",
    "            \n",
    "            for j in range(num_constraint):\n",
    "                cost_col = f'con_cost_{j}'  \n",
    "                if cost_col in self.state_df_id.columns:\n",
    "                    con_cost.append(self.state_df_id[cost_col].values[i])\n",
    "                else:\n",
    "                    con_cost.append(0.0) \n",
    "\n",
    "            if done == 0.0:\n",
    "                idx = self.state_df_id.index[i]\n",
    "                next_state = self.rl_cont_state_table_scaled_1.loc[idx + 1].values\n",
    "            else:\n",
    "                next_state = self.terminal_state\n",
    "\n",
    "            self.test_memory.push(state, action, obj_cost, con_cost, next_state, done)\n",
    "\n",
    "    def data_torch_loader_test(self):\n",
    "        state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch = self.test_memory.extract()\n",
    "\n",
    "        state_batch = torch.tensor(np.array(state_batch), device = self.cfg.device, dtype = torch.float)\n",
    "\n",
    "        disch_batch = list(action_batch)\n",
    "        action_batch = torch.tensor(np.array(action_batch), device = self.cfg.device, dtype = torch.long).unsqueeze(1)\n",
    "        \n",
    "        obj_cost_batch = torch.tensor(np.array(obj_cost_batch), device = self.cfg.device, dtype = torch.float)\n",
    "        con_cost_batch = [torch.tensor(np.array(cost), device = self.cfg.device, dtype=torch.float) for cost in con_cost_batch]\n",
    "        next_state_batch = torch.tensor(np.array(next_state_batch), device = self.cfg.device, dtype = torch.float)\n",
    "\n",
    "        done_batch = torch.tensor(np.array(done_batch), device = self.cfg.device, dtype = torch.float)\n",
    "        disch_batch = torch.tensor(np.array(disch_batch), device = self.cfg.device, dtype = torch.float)\n",
    "\n",
    "        return state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch, disch_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestConfig:\n",
    "    def __init__(self, constraint_num):\n",
    "        \n",
    "        self.constraint_num = constraint_num\n",
    "\n",
    "        self.memory_capacity = int(2e6)  # capacity of Replay Memory\n",
    "\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # check GPU\n",
    "        self.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_table_test = pd.read_csv('../data_output/id_table_test_v13.csv')\n",
    "rl_table_test = pd.read_csv('../data_output/rl_table_test_v13.csv')\n",
    "rl_table_test_scaled = pd.read_csv('../data_output/rl_table_test_scaled_v13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_table_test.drop(columns = ['readmission_count'], inplace = True)\n",
    "# rl_table_test_scaled.drop(columns = ['readmission_count'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the 'mortality_costs' column\n",
    "id_table_test['mortality_costs_md'] = id_table_test['mortality_costs'].copy()\n",
    "\n",
    "# Apply the conditions using vectorized operations\n",
    "discharge_action_mask = id_table_test['discharge_action'] == 1\n",
    "death_mask = id_table_test['death'] == 0\n",
    "discharge_fail_mask = id_table_test['discharge_fail'] == 1\n",
    "\n",
    "# Update 'mortality_costs_md' based on the conditions\n",
    "id_table_test.loc[discharge_action_mask & death_mask, 'mortality_costs_md'] = 0\n",
    "id_table_test.loc[discharge_action_mask & ~death_mask & discharge_fail_mask, 'mortality_costs_md'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_table_test_initial = id_table_test.copy()\n",
    "id_table_test_initial = id_table_test[id_table_test['readmission_count'] == 0].copy()\n",
    "# id_table_test_initial = id_table_test[(id_table_test['readmission_count'] == 4) | (id_table_test['readmission_count'] == 5)].copy()\n",
    "# id_table_test_initial = id_table_test_initial[id_table_test_initial['epoch'] == 1].copy()\n",
    "# id_table_test_initial = id_table_test[id_table_test['discharge_action'] == 1].copy()\n",
    "# id_table_test_initial = id_table_test_initial[id_table_test_initial['discharge_fail'] == 1].copy()\n",
    "\n",
    "mv_test_initial_index = id_table_test_initial.index\n",
    "id_index_list_initial_test = mv_test_initial_index.tolist()\n",
    "\n",
    "rl_table_test_initial = rl_table_test.loc[id_index_list_initial_test].copy()\n",
    "\n",
    "rl_table_test_scaled_initial = rl_table_test_scaled.loc[id_index_list_initial_test].copy()\n",
    "\n",
    "id_table_test_initial['con_cost_0'] = id_table_test_initial['discharge_fail_costs'].copy()\n",
    "id_table_test_initial['con_cost_1'] = id_table_test_initial['los_costs_scaled'].copy()\n",
    "\n",
    "test_cfg = TestConfig(constraint_num = 2)\n",
    "terminal_state = np.zeros(rl_table_test_scaled.shape[1])\n",
    "test_data_loader = TestDataLoader(test_cfg, \n",
    "                                  id_table_test_initial, \n",
    "                                  rl_table_test_initial, \n",
    "                                  rl_table_test_scaled_initial, \n",
    "                                  id_table_test, \n",
    "                                  rl_table_test_scaled, \n",
    "                                  terminal_state)\n",
    "\n",
    "test_data_loader.data_buffer_test(num_constraint = 2)\n",
    "state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch, disch_batch = test_data_loader.data_torch_loader_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_fqi_s1 = torch.load('../model_output/ocrl_agent_s1_fqi_20250331_v0.pth', weights_only = False)\n",
    "\n",
    "agent_fqe_obj_s1 = torch.load('../model_output/ocrl_agent_s1_fqe_obj_20250331_v0.pth', weights_only = False)\n",
    "agent_fqe_con_rr_s1 = torch.load('../model_output/ocrl_agent_s1_fqe_con_rr_20250331_v0.pth', weights_only = False)\n",
    "agent_fqe_con_los_s1 = torch.load('../model_output/ocrl_agent_s1_fqe_con_los_20250331_v0.pth', weights_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrl_policy_set_1 = agent_fqi_s1.rl_policy(state_batch)\n",
    "matches_1 = sum(1 for a, b in zip(ocrl_policy_set_1, action_batch) if a == b)\n",
    "percentage_1 = matches_1/len(ocrl_policy_set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.598563022973892"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocrl_policy_set_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0的数量: 27825, 比例: 0.8227 (27825/33821)\n",
      "1的数量: 5996, 比例: 0.1773 (5996/33821)\n"
     ]
    }
   ],
   "source": [
    "zeros = (action_batch == 0).sum().item()\n",
    "ones = (action_batch == 1).sum().item()\n",
    "total = action_batch.numel()  \n",
    "\n",
    "zero_ratio = zeros / total\n",
    "one_ratio = ones / total\n",
    "\n",
    "print(f\"0的数量: {zeros}, 比例: {zero_ratio:.4f} ({zeros}/{total})\")\n",
    "print(f\"1的数量: {ones}, 比例: {one_ratio:.4f} ({ones}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0的数量: 18644, 比例: 0.551 (18644/33821)\n",
      "1的数量: 15177, 比例: 0.449 (15177/33821)\n"
     ]
    }
   ],
   "source": [
    "zeros = (ocrl_policy_set_1 == 0).sum().item()\n",
    "ones = (ocrl_policy_set_1 == 1).sum().item()\n",
    "total = ocrl_policy_set_1.numel()  \n",
    "\n",
    "zero_ratio = zeros / total\n",
    "one_ratio = ones / total\n",
    "\n",
    "print(f\"0的数量: {zeros}, 比例: {zero_ratio:.3f} ({zeros}/{total})\")\n",
    "print(f\"1的数量: {ones}, 比例: {one_ratio:.3f} ({ones}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqe_test_obj_costs_ocrl_1, fqe_test_obj_costs_ocrl_1_ub = agent_fqe_obj_s1.avg_Q_value_est(state_batch)\n",
    "fqe_test_con_disch_costs_ocrl_1, fqe_test_con_disch_costs_ocrl_1_ub = agent_fqe_con_rr_s1.avg_Q_value_est(state_batch)\n",
    "fqe_test_con_los_costs_ocrl_1, fqe_test_con_los_costs_ocrl_1_ub = agent_fqe_con_los_s1.avg_Q_value_est(state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06513223052024841"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fqe_test_obj_costs_ocrl_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1295393407344818"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fqe_test_con_disch_costs_ocrl_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.800172328948975"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fqe_test_con_los_costs_ocrl_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.602067947387695"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fqe_test_con_los_costs_ocrl_1 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent_fqi_test = ocrl_training_s1.fqi_agent_config(seed = 1)\n",
    "\n",
    "agent_fqe_obj_test = ocrl_training_s1.fqe_agent_config(id_stop = 0, \n",
    "                                                       eval_agent = agent_fqi_test, \n",
    "                                                       weight_decay = 0.0,\n",
    "                                                       eval_target = 'obj', \n",
    "                                                       seed = 1) \n",
    "\n",
    "agent_fqe_con_rr_test = ocrl_training_s1.fqe_agent_config(id_stop = 0, \n",
    "                                                          eval_agent = agent_fqi_test, \n",
    "                                                          weight_decay = 0.0,\n",
    "                                                          eval_target = 0, \n",
    "                                                          seed = 2) \n",
    "\n",
    "agent_fqe_con_los_test = ocrl_training_s1.fqe_agent_config(id_stop = 0, \n",
    "                                                           eval_agent = agent_fqi_test, \n",
    "                                                           weight_decay = 0.0,\n",
    "                                                           eval_target = 1, \n",
    "                                                           seed = 3) \n",
    "\n",
    "cum_fqe_obj = 0\n",
    "\n",
    "for i in range(len(ocrl_training_s1.fqe_obj_models_history)):\n",
    "    agent_fqi_test.policy_net.load_state_dict(ocrl_training_s1.fqi_models_history[i]['model_state']['policy_net'])\n",
    "    agent_fqe_obj_test.policy_net.load_state_dict(ocrl_training_s1.fqe_obj_models_history[i]['model_state']['policy_net'])\n",
    "    fqe_est_obj_test, fqe_est_obj_test_ub = agent_fqe_obj_test.avg_Q_value_est(state_batch)\n",
    "    print(fqe_est_obj_test)\n",
    "    cum_fqe_obj += fqe_est_obj_test\n",
    "\n",
    "cum_fqe_con_rr = 0\n",
    "\n",
    "for i in range(len(ocrl_training_s1.fqe_obj_models_history)):\n",
    "    agent_fqi_test.policy_net.load_state_dict(ocrl_training_s1.fqi_models_history[i]['model_state']['policy_net'])\n",
    "    agent_fqe_con_rr_test.policy_net.load_state_dict(ocrl_training_s1.fqe_con_models_history[0][i]['model_state']['policy_net'])\n",
    "    fqe_est_con_rr_test, fqe_est_con_rr_test_ub = agent_fqe_con_rr_test.avg_Q_value_est(state_batch)\n",
    "    print(fqe_est_con_rr_test)\n",
    "    cum_fqe_con_rr += fqe_est_con_rr_test\n",
    "\n",
    "cum_fqe_con_los = 0\n",
    "\n",
    "for i in range(len(ocrl_training_s1.fqe_obj_models_history)):\n",
    "    agent_fqi_test.policy_net.load_state_dict(ocrl_training_s1.fqi_models_history[i]['model_state']['policy_net'])\n",
    "    agent_fqe_con_los_test.policy_net.load_state_dict(ocrl_training_s1.fqe_con_models_history[1][i]['model_state']['policy_net'])\n",
    "    fqe_est_con_los_test, fqe_est_con_los_test_ub = agent_fqe_con_los_test.avg_Q_value_est(state_batch)\n",
    "    print(fqe_est_con_los_test)\n",
    "    cum_fqe_con_los += fqe_est_con_los_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(ocrl_training_s1.fqe_obj_models_history)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[cum_fqe_obj/L, \n",
    " cum_fqe_con_rr/L,\n",
    " (cum_fqe_con_los/L)*12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch, action_batch, obj_cost_batch, con_cost_batch, next_state_batch, done_batch = train_data_loader.train_memory.extract()\n",
    "state_batch = np.array(state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_fqi_s1 = torch.load('../model_output/ocrl_safe_agent_s1_fqi_20250321.pth', weights_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(state_batch.shape[0], 30000, replace = False)\n",
    "background_data = state_batch[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = agent_fqi_s1.policy_net\n",
    "model_1.eval()\n",
    "\n",
    "weights = model_1.fc1.weight.data.cpu().numpy()\n",
    "intercept = model_1.fc1.bias.data.cpu().numpy()\n",
    "\n",
    "e = shap.LinearExplainer((weights, intercept), background_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_table_train_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.zeros(state_batch.shape[0], dtype = bool)\n",
    "selected[idx] = True\n",
    "\n",
    "X_data = state_batch[~selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = e.shap_values(X_data)\n",
    "\n",
    "shap.summary_plot(shap_values, X_data, \n",
    "                  feature_names = rl_table_train_scaled.columns, \n",
    "                  plot_size = (8, 6), \n",
    "                  plot_type = 'bar',\n",
    "                  show = False)\n",
    "\n",
    "# plt.savefig(\"../Experiment Figure/shap_summ_plot_20250415.png\", \n",
    "#             dpi = 400, \n",
    "#             bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(shap_values, list):\n",
    "    num_outputs = len(shap_values)\n",
    "    print(f\"Found {num_outputs} outputs in the SHAP values\")\n",
    "    # We'll focus on the first output for visualization\n",
    "    selected_output = 0  # Change this to visualize different outputs\n",
    "    shap_values_selected = shap_values[selected_output]\n",
    "else:\n",
    "    shap_values_selected = shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_abs_shap = np.abs(shap_values_selected).mean(axis=0)\n",
    "top_features_idx = np.argsort(mean_abs_shap)[-10:]\n",
    "top_features = [rl_table_train_scaled.columns[i] for i in top_features_idx]\n",
    "\n",
    "# Calculate mean positive and negative contributions for top features\n",
    "pos_contrib = np.zeros(len(top_features_idx))\n",
    "neg_contrib = np.zeros(len(top_features_idx))\n",
    "\n",
    "for i, idx in enumerate(top_features_idx):\n",
    "    pos_values = shap_values_selected[:, idx].copy()\n",
    "    pos_values[pos_values < 0] = 0\n",
    "    pos_contrib[i] = pos_values.mean()\n",
    "    \n",
    "    neg_values = shap_values_selected[:, idx].copy()\n",
    "    neg_values[neg_values > 0] = 0\n",
    "    neg_contrib[i] = abs(neg_values.mean())\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "contrib_df = pd.DataFrame({\n",
    "    'Feature': top_features,\n",
    "    'Positive Impact': pos_contrib,\n",
    "    'Negative Impact': neg_contrib\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "contrib_df = contrib_df.sort_values('Positive Impact', ascending=True)\n",
    "contrib_df.plot(\n",
    "    x='Feature',\n",
    "    y=['Positive Impact', 'Negative Impact'],\n",
    "    kind='barh',\n",
    "    color=['green', 'red'],\n",
    "    figsize=(12, 8)\n",
    ")\n",
    "plt.title('Positive vs Negative Feature Impact', fontsize=16)\n",
    "plt.xlabel('Mean Absolute SHAP Value', fontsize=14)\n",
    "plt.ylabel('Features', fontsize=14)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../Experiment Figure/shap_pos_neg_impact_20250415.png\", dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create statistics for one output\n",
    "def create_output_stats(shap_values_output, output_idx):\n",
    "    # 2.1 Table of feature importance statistics\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': rl_table_train_scaled.columns,\n",
    "        'Mean Absolute SHAP': np.abs(shap_values_output).mean(axis=0),\n",
    "        'Mean SHAP': shap_values_output.mean(axis=0),\n",
    "        'Min SHAP': shap_values_output.min(axis=0),\n",
    "        'Max SHAP': shap_values_output.max(axis=0),\n",
    "        'Std SHAP': shap_values_output.std(axis=0)\n",
    "    })\n",
    "\n",
    "    feature_importance = feature_importance.sort_values('Mean Absolute SHAP', ascending=False)\n",
    "    feature_importance.to_csv(f\"../Experiment Figure/shap_feature_importance_output{output_idx}_20250415.csv\", index=False)\n",
    "\n",
    "    # 2.2 Table with percentage of positive and negative contributions\n",
    "    feature_contribution = pd.DataFrame({\n",
    "        'Feature': rl_table_train_scaled.columns,\n",
    "        'Positive Contribution (%)': [(shap_values_output[:, i] > 0).mean() * 100 for i in range(shap_values_output.shape[1])],\n",
    "        'Negative Contribution (%)': [(shap_values_output[:, i] < 0).mean() * 100 for i in range(shap_values_output.shape[1])],\n",
    "        'Mean Positive SHAP': [shap_values_output[:, i][shap_values_output[:, i] > 0].mean() if any(shap_values_output[:, i] > 0) else 0 \n",
    "                              for i in range(shap_values_output.shape[1])],\n",
    "        'Mean Negative SHAP': [shap_values_output[:, i][shap_values_output[:, i] < 0].mean() if any(shap_values_output[:, i] < 0) else 0 \n",
    "                              for i in range(shap_values_output.shape[1])]\n",
    "    })\n",
    "\n",
    "    feature_contribution = feature_contribution.sort_values('Mean Positive SHAP', ascending=False)\n",
    "    feature_contribution.to_csv(f\"../Experiment Figure/shap_feature_contribution_output{output_idx}_20250415.csv\", index=False)\n",
    "    \n",
    "    return feature_importance, feature_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have multiple outputs, we can analyze each one\n",
    "if isinstance(shap_values, list):\n",
    "    # Create statistics for each output\n",
    "    for i, shap_vals in enumerate(shap_values):\n",
    "        print(f\"Creating statistics for output {i}\")\n",
    "        create_output_stats(shap_vals, i)\n",
    "        \n",
    "    # Additionally, create a summary table that compares top features across outputs\n",
    "    feature_importance_all = pd.DataFrame({'Feature': rl_table_train_scaled.columns})\n",
    "    for i, shap_vals in enumerate(shap_values):\n",
    "        importance = np.abs(shap_vals).mean(axis=0)\n",
    "        feature_importance_all[f'Importance_Output_{i}'] = importance\n",
    "    \n",
    "    # Add average importance across all outputs\n",
    "    feature_importance_all['Average_Importance'] = feature_importance_all.iloc[:, 1:].mean(axis=1)\n",
    "    feature_importance_all = feature_importance_all.sort_values('Average_Importance', ascending=False)\n",
    "    feature_importance_all.to_csv(\"../Experiment Figure/shap_feature_importance_all_outputs_20250415.csv\", index=False)\n",
    "else:\n",
    "    # Just one output\n",
    "    create_output_stats(shap_values_selected, selected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a correlation heatmap of SHAP values for the selected output\n",
    "shap_corr = np.corrcoef(shap_values_selected.T)\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(shap_corr, dtype=bool))\n",
    "sns.heatmap(\n",
    "    shap_corr,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    annot=False,\n",
    "    fmt='.2f',\n",
    "    linewidths=0.5,\n",
    "    xticklabels=rl_table_train_scaled.columns,\n",
    "    yticklabels=rl_table_train_scaled.columns\n",
    ")\n",
    "plt.title(f'Correlation Between Feature SHAP Values - Output {selected_output}', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../Experiment Figure/shap_correlation_output{selected_output}_20250415.png\", dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(shap_values, list) and len(shap_values) > 1:\n",
    "    # Get top 10 features based on average importance across all outputs\n",
    "    avg_importance = np.zeros(len(rl_table_train_scaled.columns))\n",
    "    for shap_vals in shap_values:\n",
    "        avg_importance += np.abs(shap_vals).mean(axis=0)\n",
    "    avg_importance /= len(shap_values)\n",
    "    \n",
    "    top_features_overall_idx = np.argsort(avg_importance)[-10:]\n",
    "    top_features_overall = [rl_table_train_scaled.columns[i] for i in top_features_overall_idx]\n",
    "    \n",
    "    # Create a comparison bar chart\n",
    "    comparison_data = {\n",
    "        'Feature': top_features_overall\n",
    "    }\n",
    "    \n",
    "    for i, shap_vals in enumerate(shap_values):\n",
    "        importance = np.abs(shap_vals).mean(axis=0)\n",
    "        comparison_data[f'Output {i}'] = [importance[idx] for idx in top_features_overall_idx]\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.melt(id_vars=['Feature'], var_name='Output', value_name='Importance')\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', hue='Output', data=comparison_df)\n",
    "    plt.title('Feature Importance Comparison Across Outputs', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../Experiment Figure/shap_importance_comparison_20250415.png\", dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 5.2 Create a heatmap showing importance across outputs\n",
    "    heatmap_data = np.zeros((len(top_features_overall), len(shap_values)))\n",
    "    for i, shap_vals in enumerate(shap_values):\n",
    "        importance = np.abs(shap_vals).mean(axis=0)\n",
    "        for j, idx in enumerate(top_features_overall_idx):\n",
    "            heatmap_data[j, i] = importance[idx]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        cmap='viridis',\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        xticklabels=[f'Output {i}' for i in range(len(shap_values))],\n",
    "        yticklabels=top_features_overall\n",
    "    )\n",
    "    plt.title('Feature Importance Heatmap Across Outputs', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../Experiment Figure/shap_importance_heatmap_20250415.png\", dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"All SHAP visualizations and tables have been created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a modern style for better aesthetics\n",
    "plt.style.use('seaborn-whitegrid')  # Use seaborn style for cleaner look\n",
    "plt.rcParams['font.family'] = 'Arial'  # Set font to Arial\n",
    "plt.rcParams['font.size'] = 10  # Increase base font size\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    for output_idx in range(len(shap_values)):\n",
    "        plt.figure(figsize = (12, 10))  # Larger figure size for better spacing\n",
    "        shap_plot = shap.plots.beeswarm(\n",
    "            shap.Explanation(\n",
    "                values = shap_values[output_idx],\n",
    "                base_values = e.expected_value[output_idx] if isinstance(e.expected_value, list) else e.expected_value,\n",
    "                data = X_data,\n",
    "                feature_names = rl_table_train_scaled.columns\n",
    "            ),\n",
    "            alpha = 0.5,  # Add transparency to points\n",
    "            color = plt.get_cmap(\"coolwarm\"),  # Use a more distinct colormap\n",
    "            show = False,\n",
    "            color_bar = False\n",
    "        )\n",
    "        \n",
    "        # Customize plot elements\n",
    "        plt.title(f\"SHAP Value Impact - Action {output_idx}\", fontsize = 12, pad=  20)\n",
    "        plt.xlabel(\"SHAP Value (Impact on Model Output)\", fontsize = 10)\n",
    "        plt.ylabel(\"Features\", fontsize = 10)\n",
    "        \n",
    "        # Add gridlines\n",
    "        plt.grid(True, axis = 'x', linestyle = '--', alpha = 0.3, color = 'gray')\n",
    "        \n",
    "        # Adjust tick labels\n",
    "        plt.xticks(fontsize = 10)\n",
    "        plt.yticks(fontsize = 10)\n",
    "        \n",
    "        # Add a colorbar to show feature value mapping\n",
    "        sm = plt.cm.ScalarMappable(cmap = \"coolwarm\", norm = plt.Normalize(vmin = 0, vmax = 1))\n",
    "        sm.set_array([])  \n",
    "        cbar = plt.colorbar(sm, fraction = 0.03, pad = 0.02, aspect = 30)\n",
    "        cbar.set_label(\"Feature Value (Low to High)\", fontsize = 10, rotation = 270, labelpad = 20)\n",
    "        cbar.ax.tick_params(labelsize = 10)\n",
    "        \n",
    "        # Tight layout to prevent clipping\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save as both PNG and PDF for flexibility\n",
    "        plt.savefig(f\"../Experiment Figure/shap_beeswarm_output{output_idx}_20250415_1.png\", \n",
    "                    dpi = 400, \n",
    "                    bbox_inches = 'tight')\n",
    "        # plt.savefig(f\"../Experiment Figure/shap_beeswarm_output{output_idx}_20250415.pdf\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.figure(figsize = (12, 10))\n",
    "    shap_plot = shap.plots.beeswarm(\n",
    "        shap.Explanation(\n",
    "            values = shap_values,\n",
    "            base_values = e.expected_value,\n",
    "            data = X_data,\n",
    "            feature_names = rl_table_train_scaled.columns\n",
    "        ),\n",
    "        alpha = 0.7,\n",
    "        color = plt.get_cmap(\"coolwarm\")\n",
    "    )\n",
    "    \n",
    "    # Customize plot elements\n",
    "    plt.title(\"SHAP Value Impact\", fontsize = 16, pad = 20)\n",
    "    plt.xlabel(\"SHAP Value (Impact on Model Output)\", fontsize = 14)\n",
    "    plt.ylabel(\"Features\", fontsize = 14)\n",
    "    \n",
    "    # Add gridlines\n",
    "    plt.grid(True, axis = 'x', linestyle = '--', alpha = 0.3, color = 'gray')\n",
    "    \n",
    "    # Adjust tick labels\n",
    "    plt.xticks(fontsize = 12)\n",
    "    plt.yticks(fontsize = 12)\n",
    "    \n",
    "    # Add a colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap = \"coolwarm\", norm = plt.Normalize(vmin = 0, vmax = 1))\n",
    "    plt.colorbar(sm, label = \"Feature Value (Low to High)\", fraction = 0.046, pad = 0.04)\n",
    "    \n",
    "    # Tight layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save as both PNG and PDF\n",
    "    plt.savefig(\"../Experiment Figure/shap_beeswarm_20250415.png\", \n",
    "                dpi = 300, \n",
    "                bbox_inches = 'tight')\n",
    "    # plt.savefig(\"../Experiment Figure/shap_beeswarm_20250415.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
